"""
é…ç½®ç®¡ç†æ¨¡å—
æ”¯æŒç¯å¢ƒå˜é‡ã€YAML é…ç½®æ–‡ä»¶å’Œç›´æ¥é…ç½®
"""

import os
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


def load_yaml_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶ï¼Œå¸¦å‹å¥½çš„é”™è¯¯æç¤º"""
    try:
        import yaml
        with open(config_path, 'r', encoding='utf-8') as f:
            content = f.read()
            return yaml.safe_load(content) or {}
    except ImportError:
        raise RuntimeError("pyyaml not installed, run: pip install pyyaml")
    except FileNotFoundError:
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    except yaml.YAMLError as e:
        # Provide user-friendly YAML error message
        error_msg = f"\n{'='*60}\n"
        error_msg += "âŒ YAML Configuration Syntax Error!\n"
        error_msg += f"ğŸ“ File: {config_path}\n"
        error_msg += f"{'='*60}\n"
        
        if hasattr(e, 'problem_mark'):
            mark = e.problem_mark
            error_msg += f"ğŸ“ Location: Line {mark.line + 1}, Column {mark.column + 1}\n"
            
            # Show the problematic line with context
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if mark.line < len(lines):
                        error_msg += f"\nProblematic line:\n"
                        # Show context
                        start = max(0, mark.line - 2)
                        end = min(len(lines), mark.line + 3)
                        for i in range(start, end):
                            prefix = ">>> " if i == mark.line else "    "
                            error_msg += f"{prefix}{i+1:3d}| {lines[i].rstrip()}\n"
                        if mark.column > 0:
                            error_msg += f"    {' ' * (mark.column + 4)}^\n"
            except:
                pass
        
        error_msg += f"\nğŸ’¡ Common causes:\n"
        error_msg += f"   1. Inconsistent indentation (YAML requires same-level properties to have same indentation)\n"
        error_msg += f"   2. Missing space after colon (correct: 'key: value')\n"
        error_msg += f"   3. Unquoted special characters\n"
        error_msg += f"\nOriginal error: {str(e)}\n"
        error_msg += f"{'='*60}\n"
        
        raise ValueError(error_msg)


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: str = "openai"  # openai, ollama, custom
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    model: str = "gpt-4o"
    temperature: float = 0.1
    max_tokens: int = 4096
    timeout: int = 60
    
    def __post_init__(self):
        # ä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼
        if self.api_key is None:
            self.api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
        if self.api_base is None:
            self.api_base = os.getenv("LLM_API_BASE") or os.getenv("OPENAI_API_BASE")
        
        # æ ¹æ® provider è®¾ç½®é»˜è®¤ base URL
        if self.api_base is None:
            if self.provider == "openai":
                self.api_base = "https://api.openai.com/v1"
            elif self.provider == "ollama":
                self.api_base = "http://localhost:11434/v1"


@dataclass 
class IndexerConfig:
    """ç´¢å¼•å™¨é…ç½®"""
    # RAG LLM (ç”¨äº RAG æœç´¢å’Œç´¢å¼•ç”Ÿæˆ)
    rag_llm: LLMConfig = field(default_factory=LLMConfig)
    
    # Chat LLM (å¯é€‰ï¼Œç”¨äºèŠå¤©å›å¤ï¼Œå¦‚ä¸é…ç½®åˆ™ä½¿ç”¨ rag_llm)
    chat_llm: Optional[LLMConfig] = None
    
    # å‘åå…¼å®¹ï¼šllm å±æ€§æŒ‡å‘ rag_llm
    @property
    def llm(self) -> LLMConfig:
        return self.rag_llm
    
    # ç´¢å¼•ç”Ÿæˆé…ç½®
    max_tokens_per_node: int = 20000
    max_depth: int = 6
    add_node_id: bool = True
    add_node_summary: bool = True
    add_doc_description: bool = True
    
    # è¯­è¨€è®¾ç½®
    language: str = "zh"  # zh, en
    
    @classmethod
    def from_env(cls) -> "IndexerConfig":
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®"""
        llm_config = LLMConfig(
            provider=os.getenv("LLM_PROVIDER", "openai"),
            api_key=os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY"),
            api_base=os.getenv("LLM_API_BASE") or os.getenv("OPENAI_API_BASE"),
            model=os.getenv("LLM_MODEL", "gpt-4o"),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.1")),
        )
        return cls(llm=llm_config)
    
    @classmethod
    def for_openai(cls, api_key: str, model: str = "gpt-4o", **kwargs) -> "IndexerConfig":
        """åˆ›å»º OpenAI é…ç½®"""
        llm_config = LLMConfig(
            provider="openai",
            api_key=api_key,
            model=model,
            **kwargs
        )
        return cls(llm=llm_config)
    
    @classmethod
    def for_ollama(cls, model: str = "llama3", base_url: str = "http://localhost:11434/v1", **kwargs) -> "IndexerConfig":
        """åˆ›å»º Ollama é…ç½®"""
        llm_config = LLMConfig(
            provider="ollama",
            api_base=base_url,
            model=model,
            api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®çš„ API key
            **kwargs
        )
        return cls(llm=llm_config)
    
    @classmethod
    def for_custom(cls, api_key: str, api_base: str, model: str, **kwargs) -> "IndexerConfig":
        """åˆ›å»ºè‡ªå®šä¹‰ LLM é…ç½®ï¼ˆå…¼å®¹ OpenAI API çš„æœåŠ¡ï¼‰"""
        llm_config = LLMConfig(
            provider="custom",
            api_key=api_key,
            api_base=api_base,
            model=model,
            **kwargs
        )
        return cls(llm=llm_config)
    
    @classmethod
    def from_yaml(cls, config_path: str) -> "IndexerConfig":
        """ä» YAML é…ç½®æ–‡ä»¶åˆ›å»ºé…ç½®"""
        data = load_yaml_config(config_path)
        
        # è§£æ RAG LLM é…ç½® (ç”¨äº RAG æœç´¢å’Œç´¢å¼•)
        # æ”¯æŒæ–°çš„ rag_llm å’Œæ—§çš„ llm é…ç½®å
        rag_llm_data = data.get("rag_llm") or data.get("llm", {})
        rag_llm_config = LLMConfig(
            provider=rag_llm_data.get("provider", "openai"),
            api_key=rag_llm_data.get("api_key"),
            api_base=rag_llm_data.get("api_base"),
            model=rag_llm_data.get("model", "gpt-4o"),
            temperature=float(rag_llm_data.get("temperature", 0.1)),
            max_tokens=int(rag_llm_data.get("max_tokens", 4096)),
            timeout=int(rag_llm_data.get("timeout", 60)),
        )
        
        # è§£æ Chat LLM é…ç½® (å¯é€‰ï¼Œç”¨äºèŠå¤©å›å¤)
        chat_llm_config = None
        chat_llm_data = data.get("chat_llm", {})
        if chat_llm_data:
            chat_llm_config = LLMConfig(
                provider=chat_llm_data.get("provider", "openai"),
                api_key=chat_llm_data.get("api_key"),
                api_base=chat_llm_data.get("api_base"),
                model=chat_llm_data.get("model", "gpt-4o"),
                temperature=float(chat_llm_data.get("temperature", 0.7)),
                max_tokens=int(chat_llm_data.get("max_tokens", 4096)),
                timeout=int(chat_llm_data.get("timeout", 60)),
            )
        
        # è§£æç´¢å¼•å™¨é…ç½®
        indexer_data = data.get("indexer", {})
        
        return cls(
            rag_llm=rag_llm_config,
            chat_llm=chat_llm_config,
            add_node_id=indexer_data.get("add_node_id", True),
            add_node_summary=indexer_data.get("add_node_summary", True),
            add_doc_description=indexer_data.get("add_doc_description", True),
            max_depth=indexer_data.get("max_depth", 6),
        )
    
    @classmethod
    def from_file(cls, config_path: str = None) -> "IndexerConfig":
        """
        ä»é…ç½®æ–‡ä»¶åˆ›å»ºé…ç½®
        
        æŒ‰ä»¥ä¸‹é¡ºåºæŸ¥æ‰¾é…ç½®æ–‡ä»¶ï¼š
        1. æŒ‡å®šçš„ config_path
        2. config.local.yamlï¼ˆæœ¬åœ°é…ç½®ï¼Œä¸æäº¤åˆ° gitï¼‰
        3. config.yaml
        4. ç¯å¢ƒå˜é‡
        """
        # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
        search_paths = []
        if config_path:
            search_paths.append(config_path)
        
        # åœ¨å½“å‰ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
        for base_dir in [os.getcwd(), os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))]:
            search_paths.extend([
                os.path.join(base_dir, "config.local.yaml"),
                os.path.join(base_dir, "config.yaml"),
            ])
        
        for path in search_paths:
            if os.path.exists(path):
                return cls.from_yaml(path)
        
        # æ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
        return cls.from_env()


@dataclass
class WebConfig:
    """ç½‘é¡µæŠ“å–é…ç½®"""
    timeout: int = 30
    verify_ssl: bool = True
    use_llm_for_conversion: bool = True
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "WebConfig":
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(
            timeout=data.get("timeout", 30),
            verify_ssl=data.get("verify_ssl", True),
            use_llm_for_conversion=data.get("use_llm_for_conversion", True),
        )

