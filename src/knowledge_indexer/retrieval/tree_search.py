"""
å¢å¼ºç‰ˆæ ‘æœç´¢å¼•æ“
å‚è€ƒ PageIndex çš„ reasoning-based retrievalï¼Œå®ç°å¤šè½®æ¨ç†å’Œå±‚çº§æœç´¢

ç‰¹æ€§ï¼š
1. å¤šè½®æ¨ç† - å…ˆå®šä½é«˜å±‚èŠ‚ç‚¹ï¼Œå†æ·±å…¥å­èŠ‚ç‚¹
2. ç½®ä¿¡åº¦è¯„åˆ† - ä¸ºæ¯ä¸ªèŠ‚ç‚¹è¯„ä¼°ç›¸å…³æ€§
3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ - è€ƒè™‘èŠ‚ç‚¹çš„ summary å’Œçˆ¶å­å…³ç³»
4. å›æº¯éªŒè¯ - éªŒè¯é€‰æ‹©çš„èŠ‚ç‚¹æ˜¯å¦åŒ…å«ç­”æ¡ˆ
"""

import json
import logging
import time
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from ..llm.base import BaseLLM
from ..models.tree_node import DocumentIndex, TreeNode
from ..debug_utils import (
    debug_print, debug_reasoning_round, debug_rag_results,
    debug_llm_call, debug_llm_response, DebugTimer
)

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    doc_name: str
    node_id: str
    title: str
    relevance_score: float  # 0-1 çš„ç›¸å…³æ€§åˆ†æ•°
    reasoning: str  # é€‰æ‹©è¯¥èŠ‚ç‚¹çš„ç†ç”±
    path: List[str] = field(default_factory=list)  # ä»æ ¹åˆ°è¯¥èŠ‚ç‚¹çš„è·¯å¾„


@dataclass
class SearchContext:
    """æœç´¢ä¸Šä¸‹æ–‡"""
    query: str
    documents: Dict[str, DocumentIndex]
    node_maps: Dict[str, Dict[str, TreeNode]]
    max_results: int = 10
    min_relevance: float = 0.3


class TreeSearchEngine:
    """
    å¢å¼ºç‰ˆæ ‘æœç´¢å¼•æ“
    
    å®ç°ç±»ä¼¼ PageIndex çš„ reasoning-based retrievalï¼š
    1. ç¬¬ä¸€è½®ï¼šæ‰«ææ‰€æœ‰æ–‡æ¡£çš„é¡¶å±‚ç»“æ„ï¼Œé€‰æ‹©ç›¸å…³æ–‡æ¡£å’Œé«˜å±‚ç« èŠ‚
    2. ç¬¬äºŒè½®ï¼šæ·±å…¥é€‰ä¸­çš„ç« èŠ‚ï¼Œæ‰¾åˆ°å…·ä½“çš„ç›®æ ‡èŠ‚ç‚¹
    3. ç¬¬ä¸‰è½®ï¼ˆå¯é€‰ï¼‰ï¼šéªŒè¯é€‰ä¸­èŠ‚ç‚¹çš„ç›¸å…³æ€§
    """
    
    def __init__(self, llm: BaseLLM, max_rounds: int = 2):
        """
        Args:
            llm: LLM å®ä¾‹
            max_rounds: æœ€å¤§æ¨ç†è½®æ•°
        """
        self.llm = llm
        self.max_rounds = max_rounds
    
    async def search(self, context: SearchContext) -> List[SearchResult]:
        """
        æ‰§è¡Œå¤šè½®æ ‘æœç´¢

        Args:
            context: æœç´¢ä¸Šä¸‹æ–‡

        Returns:
            List[SearchResult]: æ’åºåçš„æœç´¢ç»“æœ
        """
        if not context.documents:
            return []

        # DEBUG: å¼€å§‹æ ‘æœç´¢
        debug_print(
            "ğŸŒ³ æ ‘æœç´¢å¼•æ“å¼€å§‹",
            {
                "æŸ¥è¯¢": context.query,
                "æ–‡æ¡£æ•°é‡": len(context.documents),
                "æœ€å¤§ç»“æœæ•°": context.max_results,
                "æœ€å°ç›¸å…³åº¦": context.min_relevance,
                "æœ€å¤§æ¨ç†è½®æ•°": self.max_rounds
            },
            level="start"
        )

        search_start = time.time()

        # ç¬¬ä¸€è½®ï¼šé«˜å±‚æ‰«æ
        logger.info(f"Round 1: High-level scan for query: {context.query}")
        debug_print("ğŸ” ç¬¬ä¸€è½®: é«˜å±‚æ‰«æ", {"ç›®æ ‡": "è¯†åˆ«ç›¸å…³æ–‡æ¡£å’Œé¡¶å±‚ç« èŠ‚"}, level="search")

        high_level_candidates = await self._round1_high_level_scan(context)

        # DEBUG: ç¬¬ä¸€è½®ç»“æœ
        debug_print(
            "ğŸ“‹ ç¬¬ä¸€è½®ç»“æœ",
            {
                "å€™é€‰æ•°é‡": len(high_level_candidates),
                "å€™é€‰åˆ—è¡¨": high_level_candidates[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            },
            level="result"
        )

        if not high_level_candidates:
            logger.warning("No candidates found in round 1")
            debug_print("âš ï¸ ç¬¬ä¸€è½®æœªæ‰¾åˆ°å€™é€‰èŠ‚ç‚¹", level="warning")
            return []

        # ç¬¬äºŒè½®ï¼šæ·±å…¥æœç´¢
        logger.info(f"Round 2: Deep search in {len(high_level_candidates)} candidates")
        debug_print(
            "ğŸ” ç¬¬äºŒè½®: æ·±å…¥æœç´¢",
            {"å€™é€‰æ•°é‡": len(high_level_candidates), "ç›®æ ‡": "åœ¨å€™é€‰èŠ‚ç‚¹ä¸­æ‰¾åˆ°æœ€å…·ä½“çš„ç­”æ¡ˆä½ç½®"},
            level="search"
        )

        detailed_results = await self._round2_deep_search(context, high_level_candidates)

        # æ’åºå’Œè¿‡æ»¤
        results = sorted(detailed_results, key=lambda x: x.relevance_score, reverse=True)
        results = [r for r in results if r.relevance_score >= context.min_relevance]
        final_results = results[:context.max_results]

        # DEBUG: æœ€ç»ˆç»“æœ
        search_duration = time.time() - search_start
        debug_print(
            "ğŸ æ ‘æœç´¢å®Œæˆ",
            {
                "æ€»è€—æ—¶": f"{search_duration:.2f}ç§’",
                "åŸå§‹ç»“æœæ•°": len(detailed_results),
                "è¿‡æ»¤åç»“æœæ•°": len(results),
                "è¿”å›ç»“æœæ•°": len(final_results),
                "æœ€ç»ˆç»“æœ": [
                    {
                        "æ–‡æ¡£": r.doc_name,
                        "èŠ‚ç‚¹": r.node_id,
                        "æ ‡é¢˜": r.title,
                        "åˆ†æ•°": round(r.relevance_score, 4),
                        "ç†ç”±": r.reasoning[:100] if r.reasoning else ""
                    }
                    for r in final_results
                ]
            },
            level="end"
        )

        return final_results
    
    def search_sync(self, context: SearchContext) -> List[SearchResult]:
        """åŒæ­¥ç‰ˆæœ¬çš„æœç´¢"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import nest_asyncio
                nest_asyncio.apply()
                return loop.run_until_complete(self.search(context))
            else:
                return loop.run_until_complete(self.search(context))
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.search(context))
            finally:
                loop.close()
    
    async def _round1_high_level_scan(self, context: SearchContext) -> List[Dict]:
        """
        ç¬¬ä¸€è½®ï¼šé«˜å±‚æ‰«æ

        åªçœ‹æ–‡æ¡£æè¿°å’Œé¡¶å±‚ç« èŠ‚æ ‡é¢˜+æ‘˜è¦ï¼Œé€‰æ‹©å¯èƒ½ç›¸å…³çš„åŒºåŸŸ
        """
        # æ„å»ºé«˜å±‚ç»“æ„è§†å›¾
        high_level_view = self._build_high_level_view(context.documents)

        prompt = f"""You are an expert document retrieval system. Given a question and document structures, identify the most relevant sections.

## Question
{context.query}

## Documents Overview
{json.dumps(high_level_view, indent=2, ensure_ascii=False)}

## Task
Analyze each document and its top-level sections. For each potentially relevant section, assess how likely it contains information to answer the question.

Consider:
1. Document title and description - does this document likely contain the answer?
2. Section titles and summaries - which sections are most relevant?
3. Think about what specific information the question is asking for

## Response Format
Return a JSON object:
```json
{{
    "analysis": "<Brief analysis of which documents/sections are relevant and why>",
    "candidates": [
        {{
            "doc_name": "document_name",
            "node_id": "section_id",
            "relevance": 0.9,
            "reason": "This section likely contains..."
        }}
    ]
}}
```

Select up to 5 most relevant sections. Be selective - only include sections with relevance >= 0.5.
Return ONLY the JSON, no other text."""

        # DEBUG: è®°å½•ç¬¬ä¸€è½®æ¨ç†è¯·æ±‚
        debug_reasoning_round(
            round_num=1,
            candidates_count=sum(len(doc.get('sections', [])) for doc in high_level_view.get('documents', [])),
            prompt=prompt,
            response="(ç­‰å¾… LLM å“åº”...)"
        )

        try:
            start_time = time.time()
            response = self.llm.complete(prompt, temperature=0.1)
            duration = time.time() - start_time

            # DEBUG: è®°å½•ç¬¬ä¸€è½®æ¨ç†å“åº”
            debug_reasoning_round(
                round_num=1,
                candidates_count=sum(len(doc.get('sections', [])) for doc in high_level_view.get('documents', [])),
                prompt=f"(å·²è®°å½•ï¼Œè€—æ—¶ {duration:.2f}ç§’)",
                response=response.content
            )

            result = self._parse_json_response(response.content)
            return result.get('candidates', [])
        except Exception as e:
            logger.error(f"Round 1 search failed: {e}")
            debug_print(f"âŒ ç¬¬ä¸€è½®æ¨ç†å¤±è´¥: {e}", level="error")
            # Fallback: è¿”å›æ‰€æœ‰é¡¶å±‚èŠ‚ç‚¹
            return self._fallback_candidates(context.documents)
    
    async def _round2_deep_search(
        self,
        context: SearchContext,
        candidates: List[Dict]
    ) -> List[SearchResult]:
        """
        ç¬¬äºŒè½®ï¼šæ·±å…¥æœç´¢

        å¯¹æ¯ä¸ªå€™é€‰åŒºåŸŸè¿›è¡Œè¯¦ç»†æœç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„å…·ä½“èŠ‚ç‚¹
        """
        results = []

        for idx, candidate in enumerate(candidates):
            doc_name = candidate.get('doc_name')
            node_id = candidate.get('node_id')
            base_relevance = candidate.get('relevance', 0.5)

            # DEBUG: å¤„ç†æ¯ä¸ªå€™é€‰
            debug_print(
                f"ğŸ” ç¬¬äºŒè½®: å¤„ç†å€™é€‰ {idx + 1}/{len(candidates)}",
                {"æ–‡æ¡£": doc_name, "èŠ‚ç‚¹": node_id, "åŸºç¡€ç›¸å…³åº¦": base_relevance},
                level="search"
            )

            if doc_name not in context.node_maps:
                debug_print(f"âš ï¸ æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}", level="warning")
                continue

            node_map = context.node_maps[doc_name]
            if node_id not in node_map:
                debug_print(f"âš ï¸ èŠ‚ç‚¹ä¸å­˜åœ¨: {node_id}", level="warning")
                continue

            node = node_map[node_id]

            # å¦‚æœèŠ‚ç‚¹æ²¡æœ‰å­èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
            if not node.children:
                debug_print(
                    f"ğŸ“Œ å¶å­èŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨",
                    {"æ ‡é¢˜": node.title, "ç›¸å…³åº¦": base_relevance},
                    level="info"
                )
                results.append(SearchResult(
                    doc_name=doc_name,
                    node_id=node_id,
                    title=node.title,
                    relevance_score=base_relevance,
                    reasoning=candidate.get('reason', ''),
                    path=[node.title]
                ))
                continue

            # æœ‰å­èŠ‚ç‚¹ï¼Œæ·±å…¥æœç´¢
            subtree_view = self._build_subtree_view(node, doc_name)

            prompt = f"""You are searching within a document section to find the most specific answer location.

## Question
{context.query}

## Current Section
Title: {node.title}
Summary: {node.summary or 'No summary'}

## Subsections
{json.dumps(subtree_view, indent=2, ensure_ascii=False)}

## Task
Find the most specific subsection(s) that contain the answer.
- If the answer is in a specific subsection, select that subsection
- If the answer spans multiple subsections or is in the parent section itself, select the parent
- Consider both title and summary when making decisions

## Response Format
```json
{{
    "selected_nodes": [
        {{
            "node_id": "specific_node_id",
            "relevance": 0.95,
            "reason": "This subsection specifically discusses..."
        }}
    ]
}}
```

Return ONLY the JSON."""

            # DEBUG: è®°å½•ç¬¬äºŒè½®æ¨ç†
            debug_reasoning_round(
                round_num=2,
                candidates_count=len(node.children),
                prompt=prompt,
                response="(ç­‰å¾… LLM å“åº”...)"
            )

            try:
                start_time = time.time()
                response = self.llm.complete(prompt, temperature=0.1)
                duration = time.time() - start_time

                # DEBUG: è®°å½•å“åº”
                debug_reasoning_round(
                    round_num=2,
                    candidates_count=len(node.children),
                    prompt=f"(å·²è®°å½•ï¼Œè€—æ—¶ {duration:.2f}ç§’)",
                    response=response.content
                )

                sub_result = self._parse_json_response(response.content)

                for selected in sub_result.get('selected_nodes', []):
                    sel_node_id = selected.get('node_id')
                    if sel_node_id in node_map:
                        sel_node = node_map[sel_node_id]
                        debug_print(
                            f"âœ… é€‰ä¸­å­èŠ‚ç‚¹",
                            {"èŠ‚ç‚¹": sel_node_id, "æ ‡é¢˜": sel_node.title, "ç›¸å…³åº¦": selected.get('relevance')},
                            level="success"
                        )
                        results.append(SearchResult(
                            doc_name=doc_name,
                            node_id=sel_node_id,
                            title=sel_node.title,
                            relevance_score=selected.get('relevance', base_relevance),
                            reasoning=selected.get('reason', ''),
                            path=self._get_node_path(sel_node_id, node_map)
                        ))
                    else:
                        # å¦‚æœè¿”å›çš„ node_id æ— æ•ˆï¼Œä½¿ç”¨çˆ¶èŠ‚ç‚¹
                        debug_print(
                            f"âš ï¸ è¿”å›çš„èŠ‚ç‚¹IDæ— æ•ˆï¼Œä½¿ç”¨çˆ¶èŠ‚ç‚¹",
                            {"æ— æ•ˆID": sel_node_id, "ä½¿ç”¨": node_id},
                            level="warning"
                        )
                        results.append(SearchResult(
                            doc_name=doc_name,
                            node_id=node_id,
                            title=node.title,
                            relevance_score=base_relevance * 0.8,
                            reasoning=candidate.get('reason', ''),
                            path=[node.title]
                        ))

            except Exception as e:
                logger.warning(f"Round 2 deep search failed for {node_id}: {e}")
                debug_print(f"âŒ ç¬¬äºŒè½®æ·±å…¥æœç´¢å¤±è´¥: {e}", {"èŠ‚ç‚¹": node_id}, level="error")
                # Fallback: ä½¿ç”¨å€™é€‰èŠ‚ç‚¹æœ¬èº«
                results.append(SearchResult(
                    doc_name=doc_name,
                    node_id=node_id,
                    title=node.title,
                    relevance_score=base_relevance,
                    reasoning=candidate.get('reason', ''),
                    path=[node.title]
                ))

        return results
    
    def _build_high_level_view(self, documents: Dict[str, DocumentIndex]) -> Dict:
        """æ„å»ºé«˜å±‚è§†å›¾ï¼ˆåªåŒ…å«æ–‡æ¡£æè¿°å’Œé¡¶å±‚ç« èŠ‚ï¼‰"""
        view = {"documents": []}
        
        for doc_name, index in documents.items():
            doc_view = {
                "doc_name": doc_name,
                "title": index.title,
                "description": index.description[:500] if index.description else "",
                "sections": []
            }
            
            # åªåŒ…å«é¡¶å±‚èŠ‚ç‚¹ï¼ˆlevel 1-2ï¼‰
            for node in index.root_nodes:
                section_view = {
                    "node_id": node.node_id,
                    "title": node.title,
                    "level": node.level,
                    "summary": node.summary[:200] if node.summary else "",
                    "has_children": len(node.children) > 0
                }
                doc_view["sections"].append(section_view)
                
                # ä¹ŸåŒ…å«äºŒçº§èŠ‚ç‚¹
                for child in node.children[:5]:  # é™åˆ¶æ•°é‡
                    child_view = {
                        "node_id": child.node_id,
                        "title": child.title,
                        "level": child.level,
                        "summary": child.summary[:150] if child.summary else "",
                        "has_children": len(child.children) > 0
                    }
                    doc_view["sections"].append(child_view)
            
            view["documents"].append(doc_view)
        
        return view
    
    def _build_subtree_view(self, node: TreeNode, doc_name: str) -> Dict:
        """æ„å»ºå­æ ‘è§†å›¾"""
        def node_to_dict(n: TreeNode, depth: int = 0) -> Dict:
            result = {
                "node_id": n.node_id,
                "title": n.title,
                "level": n.level,
                "summary": n.summary[:200] if n.summary else "",
            }
            if n.children and depth < 2:  # é™åˆ¶æ·±åº¦
                result["children"] = [node_to_dict(c, depth + 1) for c in n.children]
            return result
        
        return {
            "parent": {
                "node_id": node.node_id,
                "title": node.title,
                "summary": node.summary[:200] if node.summary else "",
            },
            "children": [node_to_dict(c) for c in node.children]
        }
    
    def _get_node_path(self, node_id: str, node_map: Dict[str, TreeNode]) -> List[str]:
        """è·å–èŠ‚ç‚¹è·¯å¾„"""
        # ç®€åŒ–å®ç°ï¼šåªè¿”å›èŠ‚ç‚¹æ ‡é¢˜
        if node_id in node_map:
            return [node_map[node_id].title]
        return []
    
    def _fallback_candidates(self, documents: Dict[str, DocumentIndex]) -> List[Dict]:
        """Fallbackï¼šè¿”å›æ‰€æœ‰é¡¶å±‚èŠ‚ç‚¹ä½œä¸ºå€™é€‰"""
        candidates = []
        for doc_name, index in documents.items():
            for node in index.root_nodes[:3]:  # é™åˆ¶æ•°é‡
                candidates.append({
                    "doc_name": doc_name,
                    "node_id": node.node_id,
                    "relevance": 0.5,
                    "reason": "Fallback selection"
                })
        return candidates
    
    def _parse_json_response(self, content: str) -> Dict:
        """è§£æ LLM è¿”å›çš„ JSON"""
        content = content.strip()
        
        # ç§»é™¤ markdown ä»£ç å—
        if content.startswith('```'):
            lines = content.split('\n')
            if lines[0].strip().startswith('```'):
                lines = lines[1:]
            if lines and lines[-1].strip() == '```':
                lines = lines[:-1]
            content = '\n'.join(lines)
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass
        
        # å°è¯•æå– JSON
        start = content.find('{')
        end = content.rfind('}') + 1
        if start >= 0 and end > start:
            try:
                return json.loads(content[start:end])
            except json.JSONDecodeError:
                pass
        
        logger.warning(f"Failed to parse JSON: {content[:200]}")
        return {}


class SimpleTreeSearch:
    """
    ç®€åŒ–ç‰ˆæ ‘æœç´¢ï¼ˆå•è½®ï¼‰
    
    ç”¨äºå¯¹æ¯”æµ‹è¯•æˆ–ä½œä¸º fallback
    """
    
    def __init__(self, llm: BaseLLM):
        self.llm = llm
    
    def search(
        self, 
        query: str, 
        tree_structure: Dict,
        max_results: int = 5
    ) -> List[Dict]:
        """å•è½®æ ‘æœç´¢"""
        prompt = f"""You are given a question and tree structures of multiple documents.
Each node contains a node_id, title, level, and summary.
Your task is to find all nodes that are likely to contain the answer to the question.

Question: {query}

Document tree structures:
{json.dumps(tree_structure, indent=2, ensure_ascii=False)}

Important: 
- Pay attention to node summaries - they describe what information each section contains
- Select the most SPECIFIC nodes that would answer the question
- Include child nodes if they are more specific than parent nodes
- Consider nodes related to: contact info, personal details, work history, skills, etc.

Please reply in the following JSON format:
{{
    "thinking": "<Your step-by-step reasoning about which nodes are relevant>",
    "node_list": [
        {{"doc_name": "document_name", "node_id": "node_id_1", "relevance": 0.9}},
        {{"doc_name": "document_name", "node_id": "node_id_2", "relevance": 0.8}}
    ]
}}

Select up to {max_results} most relevant nodes.
Return ONLY the JSON structure."""

        try:
            response = self.llm.complete(prompt, temperature=0.1)
            content = response.content.strip()
            
            # è§£æ JSON
            if content.startswith('```'):
                lines = content.split('\n')
                if lines[0].strip().startswith('```'):
                    lines = lines[1:]
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]
                content = '\n'.join(lines)
            
            try:
                result = json.loads(content)
                return result.get('node_list', [])
            except json.JSONDecodeError:
                start = content.find('{')
                end = content.rfind('}') + 1
                if start >= 0 and end > start:
                    result = json.loads(content[start:end])
                    return result.get('node_list', [])
                raise
                
        except Exception as e:
            logger.error(f"Simple tree search failed: {e}")
            return []


