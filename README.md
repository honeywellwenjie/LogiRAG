<div align="center">

# LogiRAG

**Reasoning-based RAG with Tree Indexing**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)

*No Vector DB â€¢ No Chunking â€¢ Human-like Retrieval â€¢ Multi-LLM Support*

[English](README.md) | [ä¸­æ–‡](README_CN.md)

</div>

---

## ğŸ“¢ Introduction

**LogiRAG** is an open-source, reasoning-based RAG (Retrieval-Augmented Generation) system inspired by [PageIndex](https://github.com/VectifyAI/PageIndex). It builds a hierarchical tree index from documents and uses LLM reasoning to navigate and retrieve relevant contentâ€”just like how humans read documents.

### Why LogiRAG?

Traditional vector-based RAG relies on **semantic similarity**, but **similarity â‰  relevance**. When working with professional documents that require domain expertise and multi-step reasoning, similarity search often falls short.

LogiRAG uses **reasoning-based retrieval**:
1. Build a "Table of Contents" **tree structure** from documents
2. Use LLM to **reason** through the tree to find relevant sections

---

## âœ¨ Features

### Core Features (Inspired by PageIndex)
| Feature | Description |
|---------|-------------|
| ğŸš« **No Vector DB** | Uses document structure and LLM reasoning, not vector similarity |
| ğŸš« **No Chunking** | Documents organized into natural sections, not artificial chunks |
| ğŸ§  **Human-like Retrieval** | Simulates how experts navigate complex documents |
| ğŸ“Š **Explainable** | Traceable reasoning process with section references |

### ğŸš€ LogiRAG Unique Features

| Feature | Description |
|---------|-------------|
| ğŸŒ **Web Scraping** | Crawl and index web pages with multi-level link following |
| ğŸ–¥ï¸ **Web UI** | Built-in chat demo and file upload interface |
| ğŸ¤– **Multi-LLM Support** | Works with OpenAI, Ollama, DeepSeek, Azure, vLLM, LocalAI, and any OpenAI-compatible API |
| ğŸ³ **Docker Ready** | One-command deployment with Docker Compose |
| ğŸ“¤ **File Upload** | Drag-and-drop file upload with automatic indexing |
| ğŸ’¬ **Chat Demo** | Interactive chat interface with RAG debug panel |
| ğŸ“Š **Context Savings** | Shows token savings (typically 95%+ reduction) |
| ğŸ”„ **Hot Reload** | Update knowledge base without restart |

---

## ğŸ–¼ï¸ Screenshots

### Chat Demo with RAG Debug Panel

<img src="docs/images/logirag_demo.png" alt="LogiRAG Demo" width="100%">

- **Left Panel**: RAG Debug Log showing reasoning process, matched nodes, and context statistics
- **Right Panel**: Chat interface with knowledge-based responses
- **99%+ Token Savings**: Only relevant sections are sent to LLM

### File Upload Interface

<img src="docs/images/logirag_upload.png" alt="LogiRAG Upload" width="100%">

- Drag-and-drop file upload
- Optional LLM summary generation
- Automatic indexing

---

## ğŸ› ï¸ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/honeywellwenjie/LogiRAG.git
cd LogiRAG
```

### 2. Configure LLM

```bash
cp config.example.yaml config.yaml
# Edit config.yaml with your LLM settings
```

**Example configurations:**

<details>
<summary>OpenAI</summary>

```yaml
llm:
  provider: openai
  api_key: "sk-your-api-key"
  api_base: "https://api.openai.com/v1"
  model: "gpt-4o"
```
</details>

<details>
<summary>Ollama (Local)</summary>

```yaml
llm:
  provider: ollama
  api_base: "http://localhost:11434/v1"
  model: "llama3"
```
</details>

<details>
<summary>DeepSeek</summary>

```yaml
llm:
  provider: openai
  api_key: "sk-your-deepseek-key"
  api_base: "https://api.deepseek.com/v1"
  model: "deepseek-chat"
```
</details>

<details>
<summary>vLLM / LocalAI / LM Studio</summary>

```yaml
llm:
  provider: openai
  api_key: "not-needed"
  api_base: "http://localhost:8000/v1"
  model: "your-local-model"
```
</details>

### 3. Start with Docker (Recommended)

```bash
./tools/restart-rag.sh
```

Or manually:

```bash
cd server
docker compose up -d
```

### 4. Access Web Interface

| Interface | URL | Description |
|-----------|-----|-------------|
| ğŸ’¬ Chat Demo | http://localhost:3003/demo | Interactive chat with RAG |
| ğŸ“¤ Upload | http://localhost:3003/upload | Upload knowledge files |
| ğŸ“Š Stats | http://localhost:3003/fstats | Knowledge base statistics |
| â¤ï¸ Health | http://localhost:3003/health | Service health check |

---

## ğŸ“š Usage

### Index a Markdown File

```bash
python tools/run_indexer.py --md_path /path/to/document.md
```

### Index a Web Page

```bash
# Single page
python tools/run_web_indexer.py --url https://example.com

# Crawl with depth
python tools/run_web_indexer.py --url https://example.com --level 2 --max-pages 50
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/query` | POST | RAG query with reasoning |
| `/upload` | POST | Upload and index files |
| `/chat` | POST | Chat with knowledge base |
| `/reload` | POST | Reload all indexes |
| `/fstats` | GET | Knowledge base statistics |
| `/health` | GET | Health check |

**Query Example:**

```bash
curl -X POST http://localhost:3003/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is LogiRAG?"}'
```

---

## ğŸ“ Project Structure

```
LogiRAG/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ knowledge_indexer/     # Core indexing library
â”‚       â”œâ”€â”€ indexer/           # Document parsing & tree building
â”‚       â”œâ”€â”€ llm/               # Multi-LLM support
â”‚       â”œâ”€â”€ retrieval/         # Reasoning-based search
â”‚       â””â”€â”€ web/               # Web scraping
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ rag_server.py          # Flask API server
â”‚   â”œâ”€â”€ Dockerfile             # Docker configuration
â”‚   â””â”€â”€ docker-compose.yml     # Docker Compose
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ run_indexer.py         # CLI for Markdown indexing
â”‚   â”œâ”€â”€ run_web_indexer.py     # CLI for web scraping
â”‚   â””â”€â”€ restart-rag.sh         # Service restart script
â”œâ”€â”€ result/                    # Generated indexes (gitignored)
â”œâ”€â”€ config.example.yaml        # Configuration template
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

### Retrieval Modes

LogiRAG supports **three retrieval modes** to balance speed, accuracy, and cost:

| Mode | Description | Best For |
|------|-------------|----------|
| `reasoning` | LLM navigates the tree structure to find relevant content | Small knowledge bases, highest accuracy |
| `vector` | Fast embedding-based similarity search | Large knowledge bases, speed priority |
| `hybrid` | Vector pre-filters candidates, then LLM reasons on filtered set | **Recommended** - balanced approach |

#### Mode Comparison

| Aspect | Reasoning | Vector | Hybrid |
|--------|-----------|--------|--------|
| Speed | âš¡ Slow | âš¡âš¡âš¡ Fast | âš¡âš¡ Medium |
| Accuracy | â­â­â­â­â­ Best | â­â­ Good | â­â­â­â­ Very Good |
| Token Cost | High | None | Medium |
| Scalability | Limited | Excellent | Good |

#### Which Mode Should I Use?

| Knowledge Base Size | Recommended Mode | Reason |
|---------------------|------------------|--------|
| **Small** (1-10 docs, <50 nodes) | `reasoning` | LLM can handle all docs; highest accuracy |
| **Medium** (10-50 docs, 50-500 nodes) | `hybrid` | Vector pre-filters; LLM reasons on candidates |
| **Large** (50+ docs, 500+ nodes) | `hybrid` or `vector` | Prevents token explosion; fast retrieval |

> **Tip**: Start with `hybrid` mode - it works well for most cases and automatically balances speed and accuracy.

### Quick Configuration Examples

We provide three example configurations for different modes:

| File | Mode | Use Case |
|------|------|----------|
| `config.example.reasoning.yaml` | Pure LLM | Small docs, max accuracy |
| `config.example.vector.yaml` | Pure Embedding | Large docs, fast search |
| `config.example.hybrid.yaml` | Hybrid | **Recommended** for most cases |

```bash
# Copy the example that fits your needs
cp config.example.hybrid.yaml config.yaml
# Edit with your API keys
```

### Full Configuration Options

```yaml
# RAG LLM Configuration (for search & indexing)
rag_llm:
  provider: openai          # openai, ollama
  api_key: "your-key"       # API key
  api_base: "https://..."   # API endpoint
  model: "gpt-4o"           # Model name
  temperature: 0.1          # Response randomness (low for accuracy)
  max_tokens: 4096          # Max response tokens
  timeout: 60               # Request timeout (seconds)

# Chat LLM Configuration (optional, for responses)
chat_llm:
  provider: openai
  model: "gpt-4o"
  temperature: 0.7          # Higher for natural responses

# Indexer Configuration
indexer:
  add_node_id: true         # Add unique node IDs
  add_node_summary: true    # Generate node summaries
  add_doc_description: true # Generate document descriptions
  max_depth: 6              # Maximum tree depth
  generate_embeddings: true # Required for vector/hybrid modes

# Embedding Configuration (for vector/hybrid modes)
embedding:
  provider: sentence_transformer  # sentence_transformer or openai
  model: "all-MiniLM-L6-v2"       # HuggingFace model name
  device: "cpu"                   # cpu, cuda, or mps

# Retrieval Configuration
retrieval:
  mode: hybrid              # reasoning, vector, or hybrid
  vector:
    enabled: true
    top_k: 20               # Candidates from vector search
    threshold: 0.3          # Minimum similarity score
  reasoning:
    enabled: true
    max_candidates: 10      # Max nodes for LLM reasoning
    max_rounds: 2           # Max search rounds
  hybrid:
    vector_weight: 0.4      # Vector score weight
    reasoning_weight: 0.6   # Reasoning score weight
  max_results: 10           # Final result count
  min_relevance: 0.3        # Minimum relevance threshold

# Web Scraping Configuration
web:
  timeout: 30               # Request timeout
  verify_ssl: true          # Verify SSL certificates
  use_llm_for_conversion: true  # Use LLM for HTMLâ†’Markdown
```

### Required Models

#### Embedding Models (Local, Auto-downloaded)

| Model | Dimensions | Speed | Quality |
|-------|------------|-------|---------|
| `all-MiniLM-L6-v2` | 384 | âš¡âš¡âš¡ Fast | â­â­â­ Good |
| `all-mpnet-base-v2` | 768 | âš¡âš¡ Medium | â­â­â­â­ Better |
| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | âš¡âš¡âš¡ Fast | â­â­â­ Good (Multilingual) |

#### LLM Options

- **OpenAI**: `gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`
- **Ollama (Local)**: `deepseek-r1:8b`, `llama3`, `mistral`
- **DeepSeek API**: `deepseek-chat`

### Installation for Hybrid Mode

```bash
# Install embedding library
pip install sentence-transformers

# (Optional) For local LLM with Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull deepseek-r1:8b
```

---

## ğŸ†š Comparison with PageIndex

| Feature | PageIndex | LogiRAG |
|---------|-----------|---------|
| Tree Indexing | âœ… | âœ… |
| Reasoning-based Retrieval | âœ… | âœ… |
| PDF Support | âœ… | âŒ (Markdown/Text) |
| Web Scraping | âŒ | âœ… |
| Multi-level Crawling | âŒ | âœ… |
| Web UI (Chat) | âŒ | âœ… |
| File Upload UI | âŒ | âœ… |
| Docker Deployment | âŒ | âœ… |
| Multi-LLM Support | OpenAI only | âœ… All OpenAI-compatible |
| Local Models | âŒ | âœ… Ollama, vLLM, etc. |
| RAG Debug Panel | âŒ | âœ… |
| Open Source | âœ… MIT | âœ… MIT |

---

## ğŸ  Local LLM Support & Limitations

LogiRAG supports local LLMs through Ollama, vLLM, LocalAI, and other OpenAI-compatible servers.

### âœ… What Works

| Feature | Local LLM Status |
|---------|------------------|
| **Index Generation** | âœ… Works well with DeepSeek-R1 (1.5B to 32B) |
| **Node Summaries** | âœ… Generated correctly |
| **Document Descriptions** | âœ… Generated correctly |
| **`<think>` Tag Handling** | âœ… Automatically cleaned for reasoning models |

### âš ï¸ Known Limitations

| Feature | Local LLM Status |
|---------|------------------|
| **RAG Retrieval** | âš ï¸ Limited accuracy |
| **Query Understanding** | âš ï¸ May misinterpret user intent |
| **JSON Response Format** | âš ï¸ Sometimes returns empty or malformed |

**Current Status:**
- Local models (tested: DeepSeek-R1 1.5B ~ 32B) can successfully generate tree indexes with summaries
- However, during RAG retrieval, the LLM's reasoning capability is limited, often resulting in:
  - Misunderstanding query intent
  - Returning empty node lists
  - Selecting irrelevant sections

### ğŸ§ª Help Us Test!

We're looking for community feedback on which local models work best for RAG retrieval:

- **Your Model**: Which local model did you test?
- **Index Generation**: Did it work?
- **RAG Retrieval**: Did it correctly match relevant nodes?
- **Configuration**: What settings worked best?

**Please submit your findings as a GitHub Issue!** Your feedback will help us:
1. Identify the minimum model size/capability for effective RAG
2. Optimize prompts for local models
3. Potentially add model-specific optimizations

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Inspired by [PageIndex](https://github.com/VectifyAI/PageIndex) by VectifyAI
- Thanks to all contributors and users

---

## â­ Star History

If you find this project useful, please consider giving it a â­!

Your star helps others discover this project and motivates continued development.

[![Star this repo](https://img.shields.io/github/stars/yourusername/LogiRAG?style=social)](https://github.com/yourusername/LogiRAG)

---

<div align="center">

**[â¬† Back to Top](#-logirag)**

Made with â¤ï¸ by the LogiRAG Community

</div>
