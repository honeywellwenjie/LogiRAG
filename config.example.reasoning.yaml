# LogiRAG Configuration - Pure Reasoning Mode (LLM Only)
# ======================================================
# This mode uses only LLM reasoning to navigate the tree structure.
# Best for: Small knowledge bases where accuracy is the priority.
#
# Pros:
#   - Highest accuracy through multi-step reasoning
#   - Best understanding of complex queries
#   - No embedding model needed
#
# Cons:
#   - Slower (requires multiple LLM calls)
#   - Higher token cost
#   - May hit token limits with large knowledge bases
#
# Usage:
#   cp config.example.reasoning.yaml config.yaml
#   # Edit your API keys below

# ===========================================
# RAG LLM Configuration (for search & indexing)
# ===========================================
rag_llm:
  provider: openai
  api_key: "your-openai-api-key"
  api_base: "https://api.openai.com/v1"
  model: "gpt-4o"
  temperature: 0.1          # Low temperature for consistent reasoning
  max_tokens: 4096
  timeout: 60

# ===========================================
# Chat LLM Configuration (for responses)
# ===========================================
chat_llm:
  provider: openai
  api_key: "your-openai-api-key"
  api_base: "https://api.openai.com/v1"
  model: "gpt-4o"
  temperature: 0.7          # Higher temperature for natural responses
  max_tokens: 4096
  timeout: 60

# ===========================================
# Indexer Configuration
# ===========================================
indexer:
  add_node_id: true
  add_node_summary: true    # LLM generates summaries for each node
  add_doc_description: true # LLM generates document description
  max_depth: 6
  generate_embeddings: false  # Not needed for pure reasoning mode

# ===========================================
# Retrieval Configuration - Reasoning Mode
# ===========================================
retrieval:
  mode: reasoning           # Pure LLM reasoning

  # Reasoning settings
  reasoning:
    enabled: true
    max_rounds: 3           # Allow more rounds for thorough search
    # max_candidates is not used in pure reasoning mode

  # Vector search disabled
  vector:
    enabled: false

  # Result settings
  max_results: 10
  min_relevance: 0.3

# ===========================================
# Web Scraping Configuration
# ===========================================
web:
  timeout: 30
  verify_ssl: true
  use_llm_for_conversion: true

# ===========================================
# Alternative LLM Configurations
# ===========================================

# --- Local Ollama ---
# rag_llm:
#   provider: ollama
#   api_key: "not-needed"
#   api_base: "http://localhost:11434/v1"
#   model: "deepseek-r1:32b"    # Larger model recommended for reasoning
#   temperature: 0.1
#   max_tokens: 4096
#   timeout: 600                # Longer timeout for local models

# --- DeepSeek API ---
# rag_llm:
#   provider: openai
#   api_key: "your-deepseek-key"
#   api_base: "https://api.deepseek.com/v1"
#   model: "deepseek-chat"
#   temperature: 0.1
