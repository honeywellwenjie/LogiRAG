# LogiRAG Configuration Example
# Copy this file to config.yaml and fill in your settings

# ===========================================
# RAG LLM Configuration (for RAG search & indexing)
# ===========================================
rag_llm:
  provider: openai
  api_key: "your-api-key-here"
  api_base: "https://api.openai.com/v1"
  model: "gpt-4o"
  temperature: 0.1
  max_tokens: 4096
  timeout: 60

# ===========================================
# Chat LLM Configuration (for chat responses)
# ===========================================
# Optional: If not specified, falls back to 'rag_llm' config above
# You can use a different (possibly stronger) model for chat responses
chat_llm:
  provider: openai
  api_key: "your-api-key-here"
  api_base: "https://api.openai.com/v1"
  model: "gpt-4o"
  temperature: 0.7
  max_tokens: 4096
  timeout: 60

# Indexer Configuration
indexer:
  add_node_id: true
  add_node_summary: true
  add_doc_description: true
  max_depth: 6
  generate_embeddings: true  # Generate embeddings for hybrid retrieval

# ===========================================
# Embedding Configuration (for hybrid retrieval)
# ===========================================
embedding:
  # Provider: sentence_transformer (local, recommended) or openai
  provider: sentence_transformer

  # Model name - for sentence_transformer, use HuggingFace model names
  # Recommended: all-MiniLM-L6-v2 (fast, 384 dims)
  # Alternative: all-mpnet-base-v2 (better quality, 768 dims)
  # Multilingual: paraphrase-multilingual-MiniLM-L12-v2
  model: "all-MiniLM-L6-v2"

  # Device: cpu, cuda, mps (Apple Silicon)
  device: "cpu"

  # For OpenAI embeddings (if provider is openai):
  # api_key: "your-api-key"  # Defaults to rag_llm.api_key
  # model: "text-embedding-3-small"  # or text-embedding-3-large

# ===========================================
# Retrieval Configuration
# ===========================================
retrieval:
  # Mode: reasoning (LLM only), vector (embedding only), hybrid (both)
  # - reasoning: Original mode, accurate but can hit token limits with many docs
  # - vector: Fast but may miss semantic nuances
  # - hybrid: Combines both - vector pre-filters, then LLM reasons on candidates
  mode: hybrid

  # Vector search settings (used in vector and hybrid modes)
  vector:
    enabled: true
    top_k: 20           # Pre-filter top-k candidates from vector search
    threshold: 0.3      # Minimum similarity score
    use_chunk_aggregation: true  # Use PageIndex-style chunk scoring
    chunk_size: 500     # Chunk size for content splitting

  # LLM reasoning settings (used in reasoning and hybrid modes)
  reasoning:
    enabled: true
    max_candidates: 10  # Max nodes for LLM to process (hybrid mode)
    max_rounds: 2       # Max search rounds

  # Hybrid fusion settings
  hybrid:
    vector_weight: 0.4      # Weight for vector scores in final ranking
    reasoning_weight: 0.6   # Weight for reasoning scores in final ranking

  # Result settings
  max_results: 10
  min_relevance: 0.3

# Web Scraping Configuration
web:
  timeout: 30
  verify_ssl: true
  use_llm_for_conversion: true

# ===========================================
# Example Configurations for Different LLMs
# ===========================================

# --- Local Ollama (DeepSeek-R1) ---
# Note: Local models work for indexing but may have limited RAG retrieval accuracy
# IMPORTANT: Docker uses network_mode: "host" on Linux, so localhost works.
#            On Mac/Windows Docker Desktop, you may need to use host.docker.internal instead.
# rag_llm:
#   provider: ollama
#   api_key: "not-needed"
#   api_base: "http://localhost:11434/v1"  # Use localhost with network_mode: host (Linux)
#   # api_base: "http://host.docker.internal:11434/v1"  # Use this for Mac/Windows Docker Desktop
#   model: "deepseek-r1:32b"
#   temperature: 0.1
#   max_tokens: 4096
#   timeout: 600  # 10 minutes for slow local models
#
# chat_llm:
#   provider: ollama
#   api_key: "not-needed"
#   api_base: "http://localhost:11434/v1"
#   model: "deepseek-r1:32b"
#   temperature: 0.7
#   max_tokens: 4096
#   timeout: 600

# --- OpenAI ---
# rag_llm:
#   provider: openai
#   api_key: "sk-xxx"
#   api_base: "https://api.openai.com/v1"
#   model: "gpt-4o"

# --- Ollama (Local) ---
# rag_llm:
#   provider: ollama
#   api_base: "http://localhost:11434/v1"
#   model: "llama3"

# --- DeepSeek API ---
# rag_llm:
#   provider: openai
#   api_key: "sk-xxx"
#   api_base: "https://api.deepseek.com/v1"
#   model: "deepseek-chat"

# --- Azure OpenAI ---
# rag_llm:
#   provider: openai
#   api_key: "your-azure-key"
#   api_base: "https://your-resource.openai.azure.com/"
#   model: "gpt-4"

# --- vLLM / LocalAI / LM Studio ---
# rag_llm:
#   provider: openai
#   api_key: "not-needed"
#   api_base: "http://localhost:8000/v1"
#   model: "your-local-model"
